# ğŸ§  í•œêµ­ì–´ ê³µê° ë©”ì‹œì§€ ìƒì„± ëª¨ë¸ (KoGPT2 ê¸°ë°˜)

**KoGPT2** ê¸°ë°˜ì˜ ê³µê° ë©”ì‹œì§€ ìƒì„± ëª¨ë¸ì…ë‹ˆë‹¤.  
ì‚¬ìš©ìê°€ ì‘ì„±í•œ ì¼ê¸°ì™€ ê°ì • ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ, ìƒí™©ì— ì–´ìš¸ë¦¬ëŠ” ë”°ëœ»í•œ ê³µê° ë¬¸ì¥ì„ ìƒì„±í•©ë‹ˆë‹¤.

---

## âœ… ì˜ˆì‹œ

**ì…ë ¥**
ê°ì •: ìŠ¬í””
ì¼ê¸°: ì˜¤ëŠ˜ ì—¬ìì¹œêµ¬ë‘ í—¤ì–´ì ¸ì„œ ë„ˆë¬´ í˜ë“¤ì–´.
ê³µê° ë©”ì‹œì§€:
**ì¶œë ¥**
ë§ˆìŒì´ ë§ì´ í˜ë“¤ì—ˆê² ë„¤. ê´œì°®ì•„, ë‹¤ ì§€ë‚˜ê°ˆ ê±°ì•¼.
---

## ğŸ“Œ ëª¨ë¸ ì •ë³´

- **ê¸°ë°˜ ëª¨ë¸**: `skt/kogpt2-base-v2`
- **í•™ìŠµ ë°ì´í„°**: ê°ì •ë³„ 5,000ê°œì”© ì´ 35,000ê°œ
- **ì§€ì› ê°ì •**:
  - ìŠ¬í””
  - í–‰ë³µ
  - ë¶„ë…¸
  - ê³µí¬
  - ë†€ëŒ
  - ì¤‘ë¦½
  - í˜ì˜¤

- **ì…ë ¥ í˜•ì‹**
ê°ì •: [ê°ì •]
ì¼ê¸°: [ì‚¬ìš©ì ì‘ì„± ë¬¸ì¥]
ê³µê° ë©”ì‹œì§€:

- **ì¶œë ¥ í˜•ì‹**
ìƒí™©ì— ì–´ìš¸ë¦¬ëŠ” í•œ ë¬¸ì¥ ê³µê° ë©”ì‹œì§€


---

## ğŸ’» ì‚¬ìš© ë°©ë²• (Python ì˜ˆì‹œ)

```python
from transformers import AutoTokenizer, AutoModelForCausalLM

tokenizer = AutoTokenizer.from_pretrained("fufckddl/KoreanEmpathyModel")
model = AutoModelForCausalLM.from_pretrained("fufckddl/KoreanEmpathyModel").to("cuda")

def generate_empathy(text, emotion):
  prompt = f"ê°ì •: {emotion}\nì¼ê¸°: {text}\nê³µê° ë©”ì‹œì§€:"
  inputs = tokenizer(prompt, return_tensors="pt").to("cuda")
  output = model.generate(
      **inputs,
      max_new_tokens=60,
      do_sample=True,
      top_p=0.95,
      temperature=0.8,
      pad_token_id=tokenizer.pad_token_id,
      eos_token_id=tokenizer.eos_token_id
  )
  response = tokenizer.decode(output[0], skip_special_tokens=True)
  return response.split("ê³µê° ë©”ì‹œì§€:")[-1].strip()

# ì‚¬ìš© ì˜ˆì‹œ
print(generate_empathy("ì˜¤ëŠ˜ ë„ˆë¬´ ì§€ì¹˜ê³  í˜ë“¤ì—ˆì–´.", "ìŠ¬í””"))```

ğŸ› ï¸ í•™ìŠµ í™˜ê²½
GPU: NVIDIA RTX 4060

í•™ìŠµ ì‹œê°„: ì•½ 2ì‹œê°„

ë°°ì¹˜ í¬ê¸°: 2

ì—í­ ìˆ˜: 3

ìµœëŒ€ í† í° ê¸¸ì´: 128

